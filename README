Step 2: Produce a Docker Compose YAML file to run the job from a Container

- Build the docker image using the command below:
>docker build -t cfpartners .

- Start the docker-compose.yml
>docker-compose up


Step 3:  Production-ready System Diagram
Below is a production-ready system diagram for designing and coding a Spark job that ingests one or multiple CSV files
into Delta Lake, produces a Docker Compose YAML file to run the job from a container, and deploys the solution on
Azure cloud service with job orchestration:

                                      +-------------------+
                                      |                   |
                                      |    Azure Blob     |
                                      |    Storage        |
                                      |                   |
                                      +-------------------+
                                                |
                                                v
                                      +-------------------+
                                      |                   |
                                      |    Spark Job      |
                                      |    (Apache Spark) |
                                      |                   |
                                      +-------------------+
                                                |
                                                v
                                      +-------------------+
                                      |                   |
                                      |   Delta Lake      |
                                      |   (Data Lake)     |
                                      |                   |
                                      +-------------------+
                                                |
                                                v
                                      +-------------------+
                                      |                   |
                                      |  Docker Container |
                                      |                   |
                                      +-------------------+
                                                |
                                                v
                                      +-------------------+
                                      |                   |
                                      | Docker Compose    |
                                      |   YAML File       |
                                      |                   |
                                      +-------------------+
                                                |
                                                v
                                      +-------------------+
                                      |                   |
                                      | Azure Container   |
                                      |  Instances (ACI)  |
                                      |  or Azure Kubernetes|
                                      |  Service (AKS)    |
                                      |                   |
                                      +-------------------+
                                                |
                                                v
                                      +-------------------+
                                      |                   |
                                      | Job Orchestration |
                                      |  (e.g. Airflow)   |
                                      |                   |
                                      +-------------------+

The system design includes the following components:

- Azure Blob Storage: This is where the CSV files are stored, and it provides scalable and cost-effective storage for
unstructured data.
- Spark Job: The Spark job is developed using Apache Spark and reads CSV files from Azure Blob Storage,
performs data transformations, and writes data into Delta Lake for ACID transactions and schema evolution.
- Docker Container: The Spark job and its dependencies are packaged into a Docker container, providing portability and
consistency across environments.
- Docker Compose YAML File: A YAML file defines the services, networks, and volumes needed to run the Spark job container,
allowing for easy deployment and scaling.
- Azure Container Instances (ACI) or Azure Kubernetes Service (AKS): The Docker container can be deployed as ACI or AKS,
providing serverless or containerized deployment options with orchestration and scaling capabilities.
- Job Orchestration: Apache Airflow can be used to schedule, monitor, and coordinate the Spark job, handling failures and
ensuring reliable execution as per the desired schedule.

In summary, the system design allows for ingesting CSV files into Delta Lake using Spark, containerizing the job with
Docker, deploying the solution on Azure with ACI or AKS, and orchestrating the job with Airflow for efficient and
reliable data processing.